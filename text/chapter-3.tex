% -*- coding: utf-8; -*-

\chapter{Computational Methods}

Depending on which of the toy models brought forward in chapter 2 for the bath and chain of particles one uses it may be really hard or even impossible to fin an analytical solution. Thus it is common to use computational methods to simulate such a system and watch its dynamical behavior in some time interval $ [a,b] $.

To be able to use a computer for such a task one first needs to make a partition of the time interval. Consider then such a partition having $ N $ points $ t_1=a, t_2, t_3, \dots, t_N=b $ all separated by a constant time step $ \Delta t = \tau $, meaning that $ t_{n+1} - t_{n} = \tau $ for all $ n\in \{1,2,3,\dots,N\} $. They can be written in terms of $ t_{1} $ as
$$ t_{n+1} = t_{1} + n\cdot \tau. $$

In the case of deterministic baths the equations to be solved consist of a system of ordinary differential equations whose graph will me approximated on the partition points.

However when one is looking at the stochastic baths all the variables of interest have to be seen as random variables which means that instead of finding an approximation to a given graph what is needed is to make sure that the statistical properties of the equations are satisfied.

Since many methods for the solution of stochastic differential equations are adaptations of the deterministic ones, it is interesting to discuss the later first.

(Given that most of the chain do not interact directly with the random variables, maybe one can also draw approximate conclusions to the problem at hand)

\section{Deterministic}

In the case of a system of ordinary differential equations, the position of each particle may be seen as a function of time $ x=x(t) $ to be approximated.

[Show the result of simulating a simple harmonic motion with each method]

\subsection{Euler's Method} 

The Euler's Method consists of approaching the evolution of the system by a Taylor series truncated at the linear terms at each step
$$\mathbf{u}(t_n+\tau) \approx \mathbf{u}(t_n) + \tau\left( \frac{d\mathbf{u}}{dt}\right)_{t=t_n},$$
the derivative is then given by the ordinary differential equation itself
$$\frac{d\mathbf{u}}{dt} = f(\mathbf{u},t)$$
(the bold font is here used to indicate vector quantities).


When the time step goes to zero, $ \tau\to 0 $, the approximate solution is known to converge (zero-stability) under certain weak assumptions on the ODEs \cite{quarteroniNumericalMathematics2007} and thus it is to be expected that for small enough $ \tau $ and a fixed time interval such a method works reasonably well. 

However, when used to simulate the motion of an oscillator subject to an elastic potential of the form $k x^2/2$, the method gives an error for the amplitude of oscillation that grows exponentially with the number of time steps in the simulation \cite{giordanoComputationalPhysics1997}.

The reason for this is that Euler's method does not conserve energy, and instead increases it exponentially. This can be seen from a simple calculation for the equations of motion of a simple harmonic oscillator. Let the equations of motion be
\begin{equation*} \label{eq:2}
\begin{aligned}
&\frac{dv}{dt} = -\frac{k}{m}x\\
&\frac{dx}{dt}=v.
\end{aligned}
\end{equation*}

Using the approximation for Euler's method one would then write
\begin{equation*}
\begin{aligned}
&v_{n+1} = v_n -\frac{k}{m}x_n \tau\\
&x_{n+1} = x_n + v_n \tau
\end{aligned}
\end{equation*}
and squaring both sides of the equations, multiplying them by $m/2$ and $k/2$, respectively, and then summing them the end result is
$$
E_{c,n+1} = E_{c,n} -k x_n v_n \tau + \frac{k}{m} \tau^2 E_{p,n}, \quad
E_{p,n+1} = E_{p,n} + k x_n v_n \tau + \frac{k}{m} \tau^2 E_{c,n};
$$
\begin{equation*}
E_{T, n+1} = E_{T,n}\left(1 + \frac{k}{m}\tau^2\right),
\end{equation*}
showing that the energy increases with a fixed ratio simply because of the approximations used.

Mathematically this problem is due to the region of "absolute stability" of the method.[?????]

It's also possible to see this using new coordinates $x' = \sqrt{k/2}x$ and $v' = \sqrt{m/2}v$, the energy being just the distance from the point $(x',v')$ to the origin in the phase space. With each new iteration, this point goes further and further away from the origin.
(In the harmonic oscillation, the trajectory in this phase space would be a circle centered at the origin).


\subsection{Runge Kutta}

Another commonly used algorithm is the Runge Kutta family of methods, mostly the classical 2nd order and 4th order methods. In this method, the increment is approximated by a series.

$$\phi = \sum_{i=0}^{m}(a_i k_i)$$

For the second order, ....

In this case, although the method does not exactly conserves energy, the energy changes very little with each step of the metho. Because of this, the phase space looks very much like a closed ellipse.

The fourht order Runge Kutta ....

In this case, the energy actually oscillates with a very small amplitude. In fact, the oscillations can only be seen by looking at simulation results with a really small scale. Due to how small this oscillations are, the system's orbit describes an almost perfect ellipse in phase space.

\subsubsection{Verlet}

The Verlet algorithm is based on an approximation to the equation for the evolution of any quantity in the classical phase space

$$\frac{d \mathcal{A}}{dt} = \frac{\partial \mathcal{A}}{\partial t} + \left\{\mathcal{A}, \mathcal{H} \right\}$$

where $\left\{\mathcal{A}, \mathcal{H} \right\}$ is the Poisson Bracket, defined by the summation

$$\left\{\mathcal{A}, \mathcal{H} \right\} = \sum_{i=1}^{d\cdot N_p} \left[ \frac{\partial \mathcal{H}}{\partial p_i}\frac{\partial \mathcal{A}}{\partial q_i} - \frac{\partial \mathcal{H}}{\partial q_i}\frac{\partial \mathcal{A}}{\partial p_i}\right] = \sum_{i=1}^{d\cdot N_p} \left[ \Dot{q_i} \frac{\partial \mathcal{A}}{\partial q_i} + \Dot{p_i}\frac{\partial \mathcal{A}}{\partial p_i}\right]$$

which runs through the $d$ degrees of freedom of each of the $N_p$ particles \cite{salinasIntroducaoFisica2013}. For the case of the density of states, Liouville's theorem states that $\frac{d \rho}{dt} = 0$.

It is possible to define a Liouville operator $\mathcal{L}$ by means of $i\mathcal{L}A = \left\{A,\mathcal{H}\right\}$, which helps find the solution using the exponential of the operator. If the partial derivative with respect to time is zero, then

$$\mathcal{A}(\mathbf{q}(t), \mathbf{p}(t)) = e^{i\mathcal{L}t} \mathcal{A}(\mathbf{q}(0), \mathbf{p}(0))$$

The idea of the Verlet Algorithm ([allentildesley, liquid simulation]) is to define two "Liouville like" operators, $i\mathcal{L}_1$ and $i\mathcal{L}_2$ corresponding to evolutions only on position and momentum, respectively, such that $i\mathcal{L} = i\mathcal{L}_1 + i\mathcal{L}_2$, and then approximate the exponential for the time evolution as being

$$e^{i\mathcal{L}\delta t} \approx e^{i\mathcal{L}_2\delta t/2}e^{i\mathcal{L}_1\delta t}e^{i\mathcal{L}_2\delta t/2} .$$

Using $\mathcal{A}$ as being the vector in the phase space and applying the approximated evolution operator, one gets the system of equations:

$$\mathbf{p}(t+\delta t/2) = \mathbf{p}(t) + \frac{\delta t}{2}\frac{d \mathbf{p}(t)}{dt}$$

$$\mathbf{q}(t) = \mathbf{q}(t) + \delta t \frac{1}{m}\mathbf{p}(t)  $$

$$\mathbf{p}(t+\delta t) = \mathbf{p}(t+\delta t/2) + \frac{\delta t}{2}\frac{d \mathbf{p}(t+\delta t)}{dt}$$

\section{Stochastic}

Computational models for solving stochastic differential equations.

[Discuss pseudo random number generators?]

\section{Euler's Method} 

The Langevin Equation can be solved by the Euler method, using the iterations:
$$ v_{n+1} = v_{n} - \tau \gamma v_{n} + \sqrt{\tau\Gamma}\xi_{n} $$
$$ x_{n+1} = x_{n} + \tau v_{n}$$

Where $\tau = \Delta t$ is the time step used in the method and $\xi_{n}$ is a sequence of random variables sampled from a gaussian distribution $\mathcal{N}(0,1)$. 
